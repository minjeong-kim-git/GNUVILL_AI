{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GNUVILL 인공지능 스터디(중급)_3주차 심화과제",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNqOVQBqY/eC3WMTIs8E+JW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minjeong-kim-git/GNUVILL_AI/blob/main/GNUVILL_%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5_%EC%8A%A4%ED%84%B0%EB%94%94(%EC%A4%91%EA%B8%89)_3%EC%A3%BC%EC%B0%A8_%EC%8B%AC%ED%99%94%EA%B3%BC%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLS1WG_D2gpv"
      },
      "source": [
        "# 심화과제. 경사 하강법으로 인자 찾아내기\n",
        "\n",
        "Tensorflow와 numpy를 활용해 경사 하강법을 구현하고, 인자를 찾아내봅시다!\n",
        "Tensorflow에는 '**GradientDescentOptimizer**'라는 경사 하강 라이브러리가 있습니다!\n",
        "이 과제의 목표는 이 라이브러리를 써보는 것, 빈칸을 채워 실습의 내용을 복습하는 데 있습니다:)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 1. 라이브러리 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ES8jVo7x717"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0fjAo-T3AKe"
      },
      "source": [
        "## 2. 데이터 셋 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH4GW1MZ28Dr"
      },
      "source": [
        "# numpy 랜덤으로 100개의 가짜 데이터 채우기 (float64 -> float32로 변환)\n",
        "x_data = np.float32(np.random.rand(2, 100))\n",
        "\n",
        "# 학습 레이블(목표값)은 아래의 식으로 산출(W = [0.1, 0.2], b = 0.3)\n",
        "y_data = np.dot([0.100, 0.200], x_data) + 0.300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHXzRL9r3uXq"
      },
      "source": [
        "## 3. 입력 데이터와 W, b를 사용해 선형 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f6XUIQC31QP"
      },
      "source": [
        "# b는 0\n",
        "b = tf.Variable(tf.zeros[[1]])\n",
        "\n",
        "# W는 1X2 형태의 웨이트 변수(균등 랜덤값으로 초기화)\n",
        "W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))\n",
        "y = tf.matmul(W, x_data) + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prxj4cYd4ver"
      },
      "source": [
        "## 4, 손실, 학습 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK6WjFh642ac"
      },
      "source": [
        "# 빈칸 1. loss 함수 정의: 3주차 실습에서 다뤘던 손실(비용) 함수랑 똑같습니다! tf.reduce_mean을 사용해봅시다\n",
        "\n",
        "# 경사하강법으로 손실 함수를 최소화(0.5는 학습률)\n",
        "optimizer = tf.train.GradientDescentOptimizer(0.5) # 경사 하강 라이브러리\n",
        "\n",
        "# 빈칸 2, 학습 오퍼레이션 정의: train이라는 변수에 optimizer를 사용해 loss가 최소화되는 지점을 구해볼게요!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDC8hTbY7BPN"
      },
      "source": [
        "## 5. 학습\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpfuZ6pe7TtU"
      },
      "source": [
        "# 모든 변수를 초기화\n",
        "init = tf.initialize_all_variables()\n",
        "\n",
        "# 세션 시작\n",
        "sess = tf.Session()\n",
        "# 빈칸 3. init 세션 시작\n",
        "\n",
        "# 200번 학습\n",
        "# 만약 step의 범위가 0에서 200까지일 때:\n",
        "  # train 세션을 시작한다.\n",
        "  # 만약 step을 20으로 나눈 나머지가 0이라면:\n",
        "    # step을 출력, W 세션 시작, b 세션 시작"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}